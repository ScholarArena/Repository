# Abstract 
大型语言模型（LLMs）在使用多智能体系统自动解决任务方面实现了显著的进步。背景
然而，大多数现有的基于LLM的多智能体方法依赖于预定义的智能体来处理简单任务，限制了多智能体协作对不同场景的适应性。问题动机
因此，我们引入了AutoAgents，这是一种创新框架，能够根据不同的任务自适应地生成和协调多个专门的智能体以构建AI团队。解决策略（顶层）
具体来说，AutoAgents通过根据任务内容动态生成所需的多个智能体以及基于生成的专家智能体为当前任务规划解决方案，耦合了任务与角色之间的关系。解决策略（具体1，动态生成）
多个专门的智能体相互协作以高效完成任务。解决策略（具体2，相互协作）
同时，该框架中加入了一个观察者角色，用于反思指定的计划和智能体的响应，并对其进行改进。解决策略（具体3，观察者）
我们在各种基准测试上的实验表明，AutoAgents比现有的多智能体方法生成更连贯和准确的解决方案。实验结果和基本结论
这强调了为不同任务分配不同角色以及团队合作的重要性，为应对复杂任务提供了新的视角。
该项目的存储库可访问 https://github.com/Link-AGI/AutoAgents。

1. **任务抽象**：在不完全信息的多方交互中，若将“可执行观测”作为策略行为的门控条件（evidence gating），则系统能显著降低 *unsupported claims*，并在相同可读性下提高主张的可验证性、覆盖度及深刻性；若用一个小而稳定的“交互语义本体”（intent ontology + evidence type ontology）把策略分解为 **Intent（为何说/做）** 与 **Skill（如何产生观测）**，则在更换工具库/领域即策略具体执行方法时，高层意图策略的迁移性能下降更小（只需替换技能库映射）；若把多轮交互压缩为“争点级状态”（issue state + evidence summary），则在保持决策一致性的同时显著降低决策输入长度与认知负担，并提升对低质量/被操控文本的鲁棒性。
2. **方法一句话**：Evidence-Conditioned Strategic Acts；Intent–Skill Factorization with Ontology；Issue-State Aggregation。
3. **Case study**：在 ICLR peer review/rebuttal 上实例化，显著降低 unsupported claims、提高 evidence coverage，并在多争点聚合上提升决策一致性/效率/深度。
4. **贡献三点**：方法论 + Data/Benchmark + 开源和在线运行。

（动机里可一笔带过现实风险：审稿中LLM滥用政策与“隐藏提示/注入”事件凸显“外部证据约束”的必要性。ICLR对LLM使用政策与安全担忧可作为现实支撑。 ([ResearchGate][1])）


## 2) IJCAI 风格 Teaser：跨领域科学问题 + 审稿是关键试验场

### 2.1 统一科学问题（需要学术的表述）

> **研究问题：在文本主导的多方交互与决策场景中（同行评议、法庭辩论、应急响应、对抗评估等），如何学习“受可执行外部观测约束”的策略性言语行为（strategic speech acts），使模型输出在语用上可读、在证据上可验证，并能在更换工具/技能库即执行方法时保持高层策略可迁移？**

核心挑战不是“生成像人的文本”，而是经典的 **words–deeds inconsistency**：模型能说得很像，但其关键主张缺乏任何外部可执行观测支撑；多轮交互中这种不一致会累积放大，尤其在工具使用、检索与推理链条被 prompt injection 影响时更严重。近期已经有针对“在同行评议/文档分析链条里用隐藏指令操控 AI 评审”的系统性研究与公开事件，凸显“可执行证据约束”的必要性。([卫报][1])

与此同时，工具学习方向（Toolformer / ToolBench / API-Bank / ToolLLM 等）证明了 LLM 能学会“何时调用什么工具”，但多数工作默认工具集合已知，且主要优化任务完成率，并未把“**观测—主张一致性**”作为中心目标，也未给出面向多方交互的结构化决策压缩。([arXiv][2])

### 2.2 为什么用 ICLR 同行评议做 case study（现实紧迫 + 数据可复现）

* ICLR 官方在 2025 年明确回应了 LLM 生成论文与 LLM 审稿质量/治理问题，并讨论了如何处理相关风险。([ICLR 博客][3])
* 近期还出现“隐藏 prompt 操控 AI 辅助评审/评阅”的新闻与论文，说明如果没有“可执行证据门控”，系统会被低成本攻击放大。([The Washington Post][4])
* 同时也有大规模实证研究/随机实验在探索 LLM 在评审流程中如何“辅助而非替代”，为你“负责任辅助”的定位提供现实依据（但你论文要把它写成 *evidence-grounded strategy learning* 的科学问题）。([adam.holter.com][5])

---

## 3) 三个科学靶点：先抽象成可检验命题，再落到科学交流的例子

为避免“太口语/太绝对”，我把靶点写成 **可检验的研究命题**（IJCAI 喜欢）。

### T1：Evidence-Conditioned Strategic Acts（证据条件化策略行为）

**命题**：在不完全信息的多方交互中，若将“可执行观测”作为策略行为的门控条件（evidence gating），则系统能显著降低 *unsupported claims*，并在相同可读性下提高主张的可验证性、覆盖度及深刻性。

* 审稿实例：强质疑（“公式错误/实验不显著/引用缺失”）必须绑定可执行观测（符号化简/统计检验/引用解析），否则只能退化为“请求澄清”。
* 指标（可量化）：Unsupported-Claim Rate、Evidence Precision、Evidence Coverage、Claim–Observation Consistency。

### T2：Intent–Skill Factorization with Ontology（语义本体约束的意图—技能因子分解）

**命题**：若用一个小而稳定的“交互语义本体”（intent ontology + evidence type ontology）把策略分解为 **Intent（为何说/做）** 与 **Skill（如何产生观测）**，则在更换工具库/领域即策略具体执行方法时，高层意图策略的迁移性能下降更小（只需替换技能库映射）。

* 审稿实例：同一 intent（检验显著性）在不同论文上只替换数据抽取与统计技能；intent policy 不需要重学 prompt。
* 指标：Cross-domain transfer drop（只换技能库/证据原语，不换 intent policy）；intent calibration（intent 与证据类型的匹配准确率）。

### T3：Issue-State Aggregation（争点状态汇聚的决策压缩）

**命题**：若把多轮交互压缩为“争点级状态”（issue state + evidence summary），则在保持决策一致性的同时显著降低决策输入长度与认知负担，并提升对低质量/被操控文本的鲁棒性。

* 审稿实例：Area Chair/Editor 不读全对话，只读每个 issue 的状态（Supported/Refuted/Inconclusive）、证据摘要、严重性。
* 指标：Decision consistency（与真实 meta-review/decision 的一致性）、Time/Token cost、Robustness under adversarial text（含隐藏 prompt/注入）。



# 1Introduction 
大型语言模型（LLM）作为多功能任务解决代理，展现出了惊人的能力，具备丰富的知识和技能。然而，它们在处理需要密集知识和推理的各种任务时仍然面临困难[Qin et al.,2023; Achiam et al., 2023; Bubeck et al.,2O23]，例如避免幻觉[Maynez et al., 202ol, 采用慢思考策略[Sloman,1996]，确保可信度[Wang et al., 2023al, 以及结合不同领域的知识和长期规划。相比之下，人类经常利用协作解决问题的好处，这使他们能够有效地合作解决各个领域中的非例行问题，并通过在专业之间分配工作量并应用多样化的视角和专业知识来提高解决方案的质量和可靠性[Nelson,2013; Roschelle and Teasley,1995;Barron,2000l。

受到协作解决问题的启发，最近的一些工作 [Wang et al.,2023c; Du et al.,2023;Liang et al., 2023; Hao et al., 2O23] 通过整合多代理讨论提高了大语言模型的任务解决能力。然而，这些多代理系统中的大多数依赖于手工制作或用户指定的代理，具有特定的角色并需要人类监督，这通常限制了协作应用的范围。此外，手动创建大量专家往往消耗大量资源。为了适应性地解决更复杂的问题，本文旨在探索一种自适应生成任务专家并通过多个专家之间的多层次协作合作完成不同任务的方法。

在这篇论文中，我们提出了AutoAgents，这是一种创新的框架，能够自适应地生成和协调多个专门的代理，以根据不同任务构建一个AI团队。图1提供了AutoAgents的高级概述。通过生成具有不同专家角色的多个代理，我们的目标是形成一个协作实体，该实体可以通过利用每个代理的互补优势来完成复杂任务。如图2所示，AutoAgents的过程分为两个关键阶段：起草阶段和执行阶段。起草阶段涉及三个预定义代理（规划者、代理观察者和计划观察者）之间的协作讨论，以综合出适合输入问题或任务的定制化代理团队和执行计划。执行阶段通过代理间的协作和反馈来完善计划，并产生最终结果。我们提出通过个体代理的自我改进和多代理的协作改进来提高代理的专业水平，并促进代理之间的知识共享。为了促进合成团队中代理之间具体的劳动分工，我们引入了一个预定义代理（行动观察者），以帮助代理团队分享信息、协调行动、达成共识以及适应环境。

将来自不同领域的异构信息综合起来通常是创意产业和其他现实场景中的关键要求。我们在图1中展示了一个具体例子，说明AutoAgents如何应对撰写关于人工智能觉醒的小说这一挑战性任务。故事规划师和研究员利用各自的专业知识合作构思故事情节，而角色开发师和作家则基于故事通过想象丰富小说内容。此外，我们对复杂任务进行了定量实验和案例研究，以证明AutoAgents的有效性。我们还进行了全面的分析，并展示了动态代理处理复杂任务的重要性、熟练代理自我完善不可或缺性以及协作对话的有效性。

总之，本文做出了以下新颖的贡献：首先，我们提出了AutoAgents，这是一种新颖的框架，能够动态合成和协调多个专家代理，为各种任务形成定制的AI团队。其次，我们在两个具有挑战性的任务上进行了严格的定量实验，并证明了AutoAgents显著提高了LLM中的知识获取和推理能力，并且优于其他生成代理框架。第三，我们通过将其应用于软件开发等各种场景中展示了AutoAgents适应复杂任务的能力。最后，我们进行了深入的研究并揭示了动态代理对于适应复杂任务的重要性以及熟练代理自我完善和协作对话的有效性。

# 2Related Work 
基于LLM的自主代理。LLM已被广泛用作可以完成特定目标的自主代理的核心控制器。Auto-GPT[Gravitas,2023]是一项早期工作，它利用LLM作为AI代理，在几个工具的帮助下可以自主实现给定的目标。然而，Auto-GPT不支持多代理协作，只能单独工作。增强LLM任务解决能力的一种方法是将不同的角色和责任分配给多个LLM，并让它们协调行动以达成共同目标。例如，BabyAGI [Nakajima, 2023] 是一个由多个基于LLM的代理组成的AI驱动的任务管理系统。一个代理根据前一个任务的目标和结果创建新任务，另一个代理对任务列表进行优先排序，还有一个代理完成任务。BabyAGI是一个具有固定顺序代理通信的多代理系统。MetaGPT[Hong et al., 2023]是一个为GPT分配不同角色以形成用于复杂任务的协作软件实体的多代理框架。它是专门针对协作软件开发的基于LLM的多代理框架。Camel [Li et al.,2023] 

代理泛化。几项研究 [Park 等，2022；Williams 等，2023] 使用 LLMs 生成用于社交模拟和流行病建模的代理，展示了这种技术如何帮助设计者在将模型部署给真实用户之前评估和改进其建模设计。同样地，ExpertPrompting [Xu 等，2023] 设计了一种方法来生成多样化的代理配置文件，这些代理可以与人类用户合作以最少的监督完成任务。然而，这种方法仍然依赖于一组限定的预定义代理，并且生成的代理仅在其配置文件上有所不同。最近，SSP [Wang 等，2023c] 和 AgentVerse [Chen 等，2023a] 提出了自动生成无限数量代理的框架。SSP 通过提供一些代理样本使 LLMs 能够为问题输入生成代理，并让这些代理解决问题。AgentVerse 通过生成的代理之间的讨论生成执行计划，并增加了循环执行的评估策略。与前两种方法不同，AutoAgents 更加强调其生成代理及其战略计划的可靠性，从而通过利用协作精炼行动和自我精炼行动的集成来增强任务执行效果，如表1所示。


# 3AutoAgents 
为了提高自主多代理组在完成目标时的有效性，AutoAgents的过程包括两个关键阶段：起草阶段和执行阶段，如图2所示。起草阶段通过分析输入的问题或任务，合成一个定制的代理团队和执行计划。执行阶段通过促进代理之间的协作和反馈来完善计划，并提供最终结果。代理之间的协作基于多代理合作的一些原则，如沟通、协调和共识。这些原则有助于代理共享信息、协调行动、达成协议并适应环境。

# 3.1 Drafting Stage 
实证证据 [Woolley et al.,2015] 表明，人类群体内的多样性促进了不同视角的产生，这增强了群体在各种任务中的表现。确定多智能体群体组成的起草阶段，在设定群体能力上限方面起着关键作用。因此，生成能够最大化群体潜力的最佳智能体团队和执行计划是至关重要的。

主流方法[Gravitas, 2023; Hong等人, 2023; Wu等人.,2023l]为自主代理分配角色描述严重依赖于人类直觉和先验知识，需要基于任务理解进行手动分配。与几个平行的发现[Xu等人., 2023; Wang等人.,2023c; Chen等人., 2023al]一致，动态设计具有不同角色的代理可以显著提高其效能。然而，在面对各种复杂问题环境时，代理和计划生成的可扩展性和合理性仍然不清楚。

一方面，生成的代理应表现出多样性以适应各种任务。另一方面，代理和计划生成应遵循某些原则，使其角色分配更加合理。因此，我们设计了三个预先定义的代理来生成代理团队和执行计划，结合人工先验知识和LLM的动态适应能力，以生成更合理的代理团队和执行计划。这三个人工预定义的代理包括规划者、代理观察者和计划观察者：

· 规划器 $\mathcal { P }$ 根据任务内容生成和优化代理团队及执行计划。 
· 代理观察者 $\mathcal { O } _ { a g e n t }$ 提供关于代理团队成员的合理性及其与任务匹配程度的建议。 
· 计划观察者 $\mathcal { O } _ { p l a n }$ 提供关于执行计划的合理性及其与任务和代理团队匹配程度的建议。

规划者生成初始的代理团队成员和特定计划，并根据与代理观察者和计划观察者的持续沟通改进代理团队和执行计划。

代理生成。规划者生成代理团队，并通过与代理观察者的双向沟通促进其持续改进。为了使规划者能够生成理性的代理，我们为单个代理的关键要素设计了一个标准格式。对于每个代理 $\mathcal { A } = \{ \mathrm { P } , \mathrm { D } , \mathrm { T } , \mathrm { S } \}$ ，规划者需要指定其提示 P、描述 D、工具集 T 和建议 S。

· 提示P为每个特定代理提供了详细且定制化的专家身份描述，其中包括概况、目标和约束。概况反映了角色或职位的专业领域知识。目标指出了该角色旨在实现的主要职责或目标。约束规定了角色在执行操作时必须遵守的限制或原则。
· 描述D提供了额外的具体身份，以帮助建立更全面的角色，制定执行计划，并检查问题。
· 工具集 T 为代理配备了可以从预定义的工具集中选择的工具。这里不为每个代理使用所有工具的原因是为了防止过多工具导致的决策困惑。
· 建议 S 为每个代理执行当前任务提供一些建议，包括但不限于明确的输出、历史信息的提取以及执行步骤的建议。

基于由规划者生成的代理列表 $\{ \mathcal { A } _ { 1 } , \mathcal { A } _ { 2 } , \cdot \cdot \cdot , \mathcal { A } _ { n } \}$，代理观察者评估每个代理的质量和适用性。代理观察者首先验证每个代理是否符合上述规范，并识别任何缺失的元素$\{ \mathrm { P } $,描述 D,工具集 $\mathrm { T } \}$。其次，代理观察者根据每个代理的描述信息和任务内容评估其与任务的兼容性。最后，代理观察者检查代理列表中是否存在冗余或缺失的角色，并相应地进行删除或添加。

在Planner和Agent Observer之间进行了$n$轮双向通信后，建立了完成任务的最佳代理列表。鉴于代理列表在任务执行中的重要作用，该框架采用预定义的代理和多个代理之间的多轮迭代对话来最终确定代理列表，从而提高执行阶段的稳定性与可靠性。

计划生成。与代理生成并行，规划者制定执行计划并通过与计划观察者的相互沟通促进其逐步改进。对于给定的任务，规划者在执行计划 $P$ 中描绘出完成该任务的具体步骤 $\{ S _ { 1 } , S _ { 2 } , \cdots S _ { n } \}$ 每个步骤 $\boldsymbol { S } _ { i }$ 都需要明确识别负责它的代理 $A _ { j }$ 以及所需的输入信息和预期输出。

计划观察者随后根据代理列表 $\{ \mathcal { A } _ { 1 } , \mathcal { A } _ { 2 } , \cdot \cdot \cdot , \overset { } { \mathcal { A } } _ { n } \}$ 和任务内容验证执行计划 $\begin{array} { l l l } { P } & { = } & { \{ S _ { 1 } , S _ { 2 } , \cdot \cdot \cdot S _ { n } \} } \end{array}$。它首先确保每个步骤都有相应的代理，并且步骤内容是连贯和简洁的。其次，它评估所有步骤是否足够、任务是否可以完成以及是否存在需要填补的空白。最后，它向规划者提供反馈，后者据此进一步完善执行计划。经过规划者和计划观察者之间的 $n$ 轮对话后，最终确定了实现任务的执行计划。

任务执行动作。规划者设计了一个执行计划，该计划自动为各种任务分配所需的代理。执行计划包括两种任务执行动作：单个代理的自我改进和多个代理的协作改进，如图3所示。自我改进使单个代理能够增强其完成某些专门任务的能力。协作改进促进了多个代理之间的知识共享，并实现了需要跨学科专业知识的任务。

# 3.2 Execution Stage 
在起草阶段，框架根据任务需求生成代理列表和执行计划。然后，框架创建相应的角色并在执行环境1中执行计划。多代理系统之间的通信和合作对于有效完成任务至关重要。本节详细阐述了多个代理之间的通信、任务执行策略和知识共享机制。

多智能体通信。许多研究[陈等人，2023a；王等人，2023c；钱等人，2023；陈等人，2O23l]已经调查了智能体之间的通信结构，以检查它们对任务性能的影响。在这个框架中，我们采用了垂直通信范式，该范式根据智能体的角色分配不同的任务。为了促进生成团队中智能体的具体劳动分工，我们引入了一个预定义的动作观察者作为团队领导者来协调执行计划。具体来说，
· 行动观察者 $\mathcal { O } _ { a c t i o n }$ 作为不同代理的任务管理者，为它们分配不同的任务，验证每个代理的执行结果，并根据执行状态动态调整执行计划。

这种精炼和沟通的机制会一直重复，直到动作观察者在执行响应上达成一致意见，或者过程达到其最大迭代限制。对于需要朝着特定目标进行迭代决策的场景，例如软件开发，纵向沟通将是一个更可取的选择。

自我精炼代理。除了代理之间的通信，单个代理的性能也对反馈结果的整体质量产生显著影响。因此，借鉴AutoGPT[Gravitas,2O23]和ReAct[Yao等人,2O22]等机制，我们为单个代理设计了一种自我精炼机制。

对于单个智能体 $\mathcal { A }$ ，在步骤 $t$ 的动作是 $a _ { t } = l _ { t } \cup p _ { t } \cup o _ { t }$，其中 $l _ { t }$ 表示语言空间中的思考或推理轨迹，这不会改变外部环境，因此没有观察反馈，$p _ { t }$ 代表任务完成的执行计划，$o _ { t }$ 包含了本次的时间完成步骤和执行输出。

如图2所示，各种类型的有用思维可以帮助制定改进计划。执行计划使代理能够预期他们未来需要采取的步骤，而执行结果构建的观察内容允许代理重新评估和增强计划安排，从而构建更精细和完整的行动。通过自我连续思考、计划、执行和反馈的循环，单个代理可以有效地执行和完成任务内容。

协作精炼行动。在协作精炼行动中，代理们以顺序方式协作改进和执行任务。每一轮协作都涉及代理之间的固定顺序轮流，他们根据当前观察生成响应。每个代理的聊天历史槽通过连接其他代理的先前发言来更新。当代理达成共识或讨论次数达到最大值时，协作自动终止。

知识共享机制。AutoAgents还促进了各个代理之间执行结果的共享，以改善沟通和反馈。然而，当代理数量众多且单个代理有更多的自我迭代时，它将生成更多的历史信息。由于LLM模型的令牌限制，它们通常无法涵盖所有信息。因此，此框架提供了短期记忆、长期记忆和动态记忆。

短期记忆主要集中在单一行动上，包括在个人行动的自我完善或协作完善阶段出现的所有中间概念、策略和结果。值得注意的是，这些行动经常以关键信息的精炼总结告终，体现了完善过程的最终阶段。

长期记忆主要关注记录各种行动的历史轨迹，主要记录每个任务的执行结果以及关键反馈信息的综合。这对于评估任务完成的全面程度是至关重要的。

动态内存主要服务于需要特别关注的动作。动作观察者能够访问长期记忆档案，巧妙地提取辅助信息，并根据动作的具体要求动态调整，以执行任务。这一过程显著提高了单个动作在任务完成中的效率。

# 4Experiments 
为了展示AutoAgents在管理一组自主代理以完成协作任务方面的性能，我们进行了定量分析，主要关注开放式问题回答任务（参见第4.1节）和Trivia创意写作任务（参见第4.2节）的结果，评估该框架在各种设置下的有效性。

实施细节。所有实验均使用GPT-4 API²进行，温度设置为O以确保可重复性。选择此模型是因为其卓越的性能，提供准确且一致的结果。通过API访问该模型极大地促进了我们与模型的交互，简化了我们的研究过程。在起草阶段，最多允许三次讨论，而在执行阶段，单个代理可以进行最多五次自我改进，多个代理可以协作改进最多五次。

# 4.1Open-ended Question Answer 
任务描述。开放式问题回答是NLP和生成式AI领域中一个关键且具有挑战性的任务。它要求AI系统对没有预设或固定答案集的问题产生连贯、详尽且类似人类的回答。[郑等人，2023]提出了MT-bench，这是一个由80个高质量收集的开放式问题组成的基准，这些问题来自常识、反事实、编码等各种类别。然后我们利用AutoAgents基于多个生成的代理生成协作答案，并将它们与GPT-4和AgentVerse [陈等人，$2023\mathrm{a}\mathrm{j}$给出的回答进行比较。

评估指标。为了以最小的评估偏差衡量开放式回答的质量，我们采用FairEval [Wang et al., 2O23bl] 和HumanEval 作为单个代理和AutoAgents的评估指标。FairEval 结合了几种方法来减轻各种偏见来源的影响，从而更好地与人类判断保持一致。对于HumanEval，我们招募了三名独立志愿者来评估两组回答——一组由AutoAgents生成，另一组由不同的模型生成——基于诸如帮助性、可靠性、准确性和全面性等标准。值得注意的是，志愿者对产生每个回答的模型身份是盲目的，确保了无偏见的评估。附录中包含了评分的具体标准。

结果。表2表明，AutoAgents在基于LLM的FairEval和人工评估中都优于单独的LLM模型。AutoAgents通过综合多个专家模型，能够为开放式问题生成更全面和细致的答案。它还可以为其答案提供更详细的解释和理由。此外，AutoAgents的表现优于AgentVerse。这种增强的有效性部分归因于AutoAgents内部代理生成、自我完善和协作完善能力的可靠性。相反，AgentVerse需要额外的任务特定调整，并且在适应开放式问题方面效果有限。更多示例见附录。

# 4.2Trivia Creative Writing 

任务描述。Trivia创意写作任务[Wang et al.,2O23]挑战了大型语言模型从其内部自我压缩的知识中检索和整合多样化信息的能力。该任务要求模型围绕给定的主题创作一个连贯的故事，同时融入$N$个琐事问题的答案。我们在两种设置下评估模型，$N = 5$ 和 $N = 10$，其中更高的$N$意味着更多的琐事问题，因此要求模型展示更广泛领域的知识。我们构建了一个基准，每个$N$包含1OO个实例，总共涵盖1ooO个琐事问题。
评估指标。借鉴[Wang et al., 2023cl]的方法，我们采用自动度量来识别事实错误并衡量模型整合不同领域知识的能力。我们对生成的输出中的每个问题与真实的目标答案进行字符串匹配。目标答案来自TriviaQA数据集[Joshi et al.,2O17]，每个问题可以有一系列的答案变体。匹配到问题的任何一个答案变体都被视为正确提及。度量分数计算为Trivia创意写作度量分数$=$正确答案提及数/Trivia问题数。

结果。表3展示了AutoAgents在知识获取方面相对于现有方法的优越性能。与不采用代理生成的标准方法相比，AutoAgents在所有实验中实现了显著的$10\%$改进。此外，AutoAgents也超越了SSP [Wang等人，2O23c]，后者虽然使用了代理生成但采用了不同的方法。AutoAgents的增强性能可以归因于其精心设计的代理生成讨论和任务执行方法，包括协作优化和自我优化。

# 4.3Further Analysis 
本节深入探讨了AutoAgents中关键组件的重要性，分别分析了自我改进动作、协作改进动作、动态记忆和观察者在Trivia创意写作任务的20个实例和额外案例研究中的草稿阶段。

协作讨论对于理性代理生成和计划分配至关重要。在起草阶段，AutoAgents中的规划者与两名观察者进行协作讨论，以确定最佳的代理列表和执行计划。图5展示了有无协作讨论的代理生成之间的对比。在没有观察者反馈的情况下，规划者倾向于仅生成程序员来完成游戏开发，忽略了游戏创作的整体过程。在观察者的输入和协调下，规划者将游戏设计专家、UI设计专家和测试专家纳入代理列表中。显然，在协作讨论下的代理生成更为全面，更符合游戏开发的实际情景。这也证实了协作讨论对于代理生成和计划分配的重要性，这将随后影响执行结果。同时，表4阐明了在没有观察者的情况下，AutoAgents的整体性能明显下降了3%。这证明了协作讨论在代理生成中的必要作用。通过协作讨论，AutoAgent显著提高了代理生成的质量，这一方面被其他生成框架在其考虑代理生成质量时所忽视。

通过自我改进增强单个代理。SelfRefinement [Madaan et al.,2023;Shinn et al.,2023; Gou et al., 2023; Chen et al., 2023b; Huang et al., 2022; Yao et al., 2022] 是一种使大语言模型能够“与自己对话”、评估自己的生成，并迭代改进其答案的技术。自我改进已被证明可以提高大语言模型在各个领域的输出准确性 [Madaan et al.,2023; Shinn et al.,2023; Gou et al., 2023; Chen et al., 2023b; Huang et al., 2022; Yao et al.,2022]。尽管AutoAgents是一个多代理协作框架，但它也需要自我改进的代理来为个别任务执行专门角色。如表4中的结果所示，在缺少自我改进行动的情况下，AutoAgents的表现下降了$3\%$。这一观察结果证实了自我改进对于增强琐事创意写作任务中的熟练度是至关重要的工具。此外，通过自我改进增强单个代理在加强整个多代理框架的完整性方面发挥着关键作用。

通过动态内存提高行动的有效性。动态内存主要解决了专门代理的需求。如图4所示，行动观察者汇集了未来任务的关键数据，利用长期记忆中存档的历史行动记录。表4阐明了没有动态内存的AutoAgents的有效性降低了1％。从动态内存中获得的基本见解被吸收到提示中，从而增强了对关键信息的理解并提高了行动的操作效率。

# 5Conclusion 
本文介绍了AutoAgents，一种旨在自动合成协作专业代理的创新框架。AutoAgents通过将任务分为起草和执行阶段，并将子任务分配给不同的代理，复制了人类团队的协作动态。我们的实验评估表明，在各种技能密集型任务中，AutoAgents的表现优于单个代理和其他组配置。此外，在软件开发中的案例研究突出了该框架的多功能性和潜在优势。AutoAgents增强了代理之间的互动与合作，革新了复杂问题解决方式。我们预计其原则可以被扩展和完善，以应用于更广泛的任务，推进辅助AI的发展。
